{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Create model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMxnZDPTtzywM9RjKDooGSr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gagan3012/pickuplines/blob/master/Create_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4-nUX1nrvdK",
        "outputId": "166cdfe0-3634-4b25-be72-004c1210f4aa"
      },
      "source": [
        "!pip install praw\n",
        "import praw"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.4.0-py3-none-any.whl (167 kB)\n",
            "\u001b[K     |████████████████████████████████| 167 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting websocket-client>=0.54.0\n",
            "  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting update-checker>=0.18\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Collecting prawcore<3,>=2.1\n",
            "  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from prawcore<3,>=2.1->praw) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\n",
            "Installing collected packages: websocket-client, update-checker, prawcore, praw\n",
            "Successfully installed praw-7.4.0 prawcore-2.3.0 update-checker-0.18.0 websocket-client-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1sHp-BvnO2k"
      },
      "source": [
        "reddit = praw.Reddit(\n",
        "    client_id=\"\",\n",
        "    client_secret=\"\",\n",
        "    user_agent=\"testscript by u/fakebot3\",\n",
        "     check_for_async=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e928DaBQvbOp",
        "outputId": "9505afa3-a9f1-4d58-9de4-4925cf123a46"
      },
      "source": [
        "print(reddit.read_only) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "EBsU_e4UvjD8",
        "outputId": "d346921c-d666-4340-af06-1c40e52fbd30"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def get_subreddit_data(sub, lim):\n",
        "    posts = [] # list of subreddits\n",
        "    SUBREDDIT = reddit.subreddit(sub) # name of subreddit'\n",
        "    LIMIT = lim # limit of posts\n",
        "\n",
        "    for post in SUBREDDIT.hot(limit=LIMIT):\n",
        "        posts.append([post.id, post.subreddit, post.title, post.url, post.selftext])\n",
        "\n",
        "    posts = pd.DataFrame(posts, columns=['id', 'subreddit', 'title', 'url', 'body'])\n",
        "    return posts\n",
        "\n",
        "df = get_subreddit_data('pickuplines', 100000)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ofnr66</td>\n",
              "      <td>pickuplines</td>\n",
              "      <td>Pick Up Lines By Name (Megathread)</td>\n",
              "      <td>https://www.reddit.com/r/pickuplines/comments/...</td>\n",
              "      <td>Please use this thread to post the name you'd ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>qaio7g</td>\n",
              "      <td>pickuplines</td>\n",
              "      <td>Are you a mask?</td>\n",
              "      <td>https://www.reddit.com/r/pickuplines/comments/...</td>\n",
              "      <td>Cos I wanna wrap your legs around my face.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>qa8fp1</td>\n",
              "      <td>pickuplines</td>\n",
              "      <td>Wow you sleep in a bed?</td>\n",
              "      <td>https://www.reddit.com/r/pickuplines/comments/...</td>\n",
              "      <td>Thats so cool I do that to, we should totally ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>qap2bm</td>\n",
              "      <td>pickuplines</td>\n",
              "      <td>If I could rearrange the periodic table of ele...</td>\n",
              "      <td>https://www.reddit.com/r/pickuplines/comments/...</td>\n",
              "      <td>I would put U and I together</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>qaqqiy</td>\n",
              "      <td>pickuplines</td>\n",
              "      <td>Are you a charger?</td>\n",
              "      <td>https://www.reddit.com/r/pickuplines/comments/...</td>\n",
              "      <td>Because I’m dying without you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>845</th>\n",
              "      <td>p57jou</td>\n",
              "      <td>pickuplines</td>\n",
              "      <td>Do you use Microsoft Excel?</td>\n",
              "      <td>https://www.reddit.com/r/pickuplines/comments/...</td>\n",
              "      <td>Because you sure can spread my sheets</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>846</th>\n",
              "      <td>p5biy7</td>\n",
              "      <td>pickuplines</td>\n",
              "      <td>I don't know from astrology but I know how the...</td>\n",
              "      <td>https://www.reddit.com/r/pickuplines/comments/...</td>\n",
              "      <td>It started with u n i</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>847</th>\n",
              "      <td>p55rql</td>\n",
              "      <td>pickuplines</td>\n",
              "      <td>Dude those pants look terrible on you</td>\n",
              "      <td>https://www.reddit.com/r/pickuplines/comments/...</td>\n",
              "      <td>Please take them off</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>848</th>\n",
              "      <td>p4zkdk</td>\n",
              "      <td>pickuplines</td>\n",
              "      <td>I know looks don’t matter…</td>\n",
              "      <td>https://www.reddit.com/r/pickuplines/comments/...</td>\n",
              "      <td>…but I can see you brought them just in case.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>849</th>\n",
              "      <td>p56elb</td>\n",
              "      <td>pickuplines</td>\n",
              "      <td>Are you a factory during the industrial revolu...</td>\n",
              "      <td>https://www.reddit.com/r/pickuplines/comments/...</td>\n",
              "      <td>Because I want to put a bunch of kids inside you</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>850 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  ...                                               body\n",
              "0    ofnr66  ...  Please use this thread to post the name you'd ...\n",
              "1    qaio7g  ...         Cos I wanna wrap your legs around my face.\n",
              "2    qa8fp1  ...  Thats so cool I do that to, we should totally ...\n",
              "3    qap2bm  ...                       I would put U and I together\n",
              "4    qaqqiy  ...                      Because I’m dying without you\n",
              "..      ...  ...                                                ...\n",
              "845  p57jou  ...              Because you sure can spread my sheets\n",
              "846  p5biy7  ...                              It started with u n i\n",
              "847  p55rql  ...                               Please take them off\n",
              "848  p4zkdk  ...      …but I can see you brought them just in case.\n",
              "849  p56elb  ...   Because I want to put a bunch of kids inside you\n",
              "\n",
              "[850 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "n4sC_G4EvmpL",
        "outputId": "e01bb29e-2417-471c-b1a7-3b04d90b0cf6"
      },
      "source": [
        "df['possible pick-up line'] = df[['title', 'body']].apply(lambda x: ' '.join(x), axis=1)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>body</th>\n",
              "      <th>possible pick-up line</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ofnr66</td>\n",
              "      <td>pickuplines</td>\n",
              "      <td>Pick Up Lines By Name (Megathread)</td>\n",
              "      <td>https://www.reddit.com/r/pickuplines/comments/...</td>\n",
              "      <td>Please use this thread to post the name you'd ...</td>\n",
              "      <td>Pick Up Lines By Name (Megathread) Please use ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>qaio7g</td>\n",
              "      <td>pickuplines</td>\n",
              "      <td>Are you a mask?</td>\n",
              "      <td>https://www.reddit.com/r/pickuplines/comments/...</td>\n",
              "      <td>Cos I wanna wrap your legs around my face.</td>\n",
              "      <td>Are you a mask? Cos I wanna wrap your legs aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>qa8fp1</td>\n",
              "      <td>pickuplines</td>\n",
              "      <td>Wow you sleep in a bed?</td>\n",
              "      <td>https://www.reddit.com/r/pickuplines/comments/...</td>\n",
              "      <td>Thats so cool I do that to, we should totally ...</td>\n",
              "      <td>Wow you sleep in a bed? Thats so cool I do tha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>qap2bm</td>\n",
              "      <td>pickuplines</td>\n",
              "      <td>If I could rearrange the periodic table of ele...</td>\n",
              "      <td>https://www.reddit.com/r/pickuplines/comments/...</td>\n",
              "      <td>I would put U and I together</td>\n",
              "      <td>If I could rearrange the periodic table of ele...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>qaqqiy</td>\n",
              "      <td>pickuplines</td>\n",
              "      <td>Are you a charger?</td>\n",
              "      <td>https://www.reddit.com/r/pickuplines/comments/...</td>\n",
              "      <td>Because I’m dying without you</td>\n",
              "      <td>Are you a charger? Because I’m dying without you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>845</th>\n",
              "      <td>p57jou</td>\n",
              "      <td>pickuplines</td>\n",
              "      <td>Do you use Microsoft Excel?</td>\n",
              "      <td>https://www.reddit.com/r/pickuplines/comments/...</td>\n",
              "      <td>Because you sure can spread my sheets</td>\n",
              "      <td>Do you use Microsoft Excel? Because you sure c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>846</th>\n",
              "      <td>p5biy7</td>\n",
              "      <td>pickuplines</td>\n",
              "      <td>I don't know from astrology but I know how the...</td>\n",
              "      <td>https://www.reddit.com/r/pickuplines/comments/...</td>\n",
              "      <td>It started with u n i</td>\n",
              "      <td>I don't know from astrology but I know how the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>847</th>\n",
              "      <td>p55rql</td>\n",
              "      <td>pickuplines</td>\n",
              "      <td>Dude those pants look terrible on you</td>\n",
              "      <td>https://www.reddit.com/r/pickuplines/comments/...</td>\n",
              "      <td>Please take them off</td>\n",
              "      <td>Dude those pants look terrible on you Please t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>848</th>\n",
              "      <td>p4zkdk</td>\n",
              "      <td>pickuplines</td>\n",
              "      <td>I know looks don’t matter…</td>\n",
              "      <td>https://www.reddit.com/r/pickuplines/comments/...</td>\n",
              "      <td>…but I can see you brought them just in case.</td>\n",
              "      <td>I know looks don’t matter… …but I can see you ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>849</th>\n",
              "      <td>p56elb</td>\n",
              "      <td>pickuplines</td>\n",
              "      <td>Are you a factory during the industrial revolu...</td>\n",
              "      <td>https://www.reddit.com/r/pickuplines/comments/...</td>\n",
              "      <td>Because I want to put a bunch of kids inside you</td>\n",
              "      <td>Are you a factory during the industrial revolu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>850 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  ...                              possible pick-up line\n",
              "0    ofnr66  ...  Pick Up Lines By Name (Megathread) Please use ...\n",
              "1    qaio7g  ...  Are you a mask? Cos I wanna wrap your legs aro...\n",
              "2    qa8fp1  ...  Wow you sleep in a bed? Thats so cool I do tha...\n",
              "3    qap2bm  ...  If I could rearrange the periodic table of ele...\n",
              "4    qaqqiy  ...   Are you a charger? Because I’m dying without you\n",
              "..      ...  ...                                                ...\n",
              "845  p57jou  ...  Do you use Microsoft Excel? Because you sure c...\n",
              "846  p5biy7  ...  I don't know from astrology but I know how the...\n",
              "847  p55rql  ...  Dude those pants look terrible on you Please t...\n",
              "848  p4zkdk  ...  I know looks don’t matter… …but I can see you ...\n",
              "849  p56elb  ...  Are you a factory during the industrial revolu...\n",
              "\n",
              "[850 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyMRq9rHd3gq"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df['possible pick-up line'], test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llh08B3-eKjL",
        "outputId": "5d6d86ab-d67f-4744-907a-6fa2e9d0bdbf"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "709    Girls call me Keanu Reeves… Because I disappea...\n",
              "827    Hey girl is your heart Afghanistan? Cause i wa...\n",
              "453    You are really hot, are you a cop? Just making...\n",
              "24     Hey girl are you my appendix? Cos the feeling ...\n",
              "316    You know the catchphrase \"Chase after your dre...\n",
              "                             ...                        \n",
              "71     I usually go out with 6 or 7 but this time I w...\n",
              "106    What do you'll think of this Someday \\n\\nIf Im...\n",
              "270    What's the difference between me and a dog? Wh...\n",
              "435    Hey girl are you anhydrous? Because I wanna ma...\n",
              "102    Help a brotha out Pick up lines for girls name...\n",
              "Name: possible pick-up line, Length: 569, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_UnW3EeeLmU"
      },
      "source": [
        "train.to_csv(r'train.txt', header=None, index=None, sep=' ', mode='a')\n",
        "test.to_csv(r'test.txt', header=None, index=None, sep=' ', mode='a')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUoEFMEuedsY",
        "outputId": "78f59da9-3f78-49df-8aaf-0becbd42c347"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 86888, done.\u001b[K\n",
            "remote: Counting objects: 100% (553/553), done.\u001b[K\n",
            "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
            "remote: Total 86888 (delta 487), reused 484 (delta 456), pack-reused 86335\u001b[K\n",
            "Receiving objects: 100% (86888/86888), 70.10 MiB | 24.01 MiB/s, done.\n",
            "Resolving deltas: 100% (62492/62492), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eolZE7u1ekoD"
      },
      "source": [
        "import os \n",
        "os.chdir(\"/content/transformers/examples/pytorch/language-modeling\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8CGdm6zet-w",
        "outputId": "91d8f4e8-07ed-4025-e49a-aa8ddfa9fee3"
      },
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.5.1-py3-none-any.whl (58 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▋                          | 10 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 20 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 40 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 51 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 58 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.9.0+cu111)\n",
            "Collecting datasets>=1.8.0\n",
            "  Downloading datasets-1.13.3-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 10.5 MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 35.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->-r requirements.txt (line 2)) (3.7.4.3)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 51.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (3.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.3.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 46.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (4.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (4.62.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (2.23.0)\n",
            "Collecting huggingface-hub<0.1.0,>=0.0.19\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.70.12.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (1.19.5)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 41.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (1.1.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets>=1.8.0->-r requirements.txt (line 3)) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets>=1.8.0->-r requirements.txt (line 3)) (3.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets>=1.8.0->-r requirements.txt (line 3)) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (21.2.0)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 47.7 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 46.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets>=1.8.0->-r requirements.txt (line 3)) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (2018.9)\n",
            "Installing collected packages: multidict, yarl, async-timeout, fsspec, aiohttp, xxhash, huggingface-hub, sentencepiece, datasets, accelerate\n",
            "Successfully installed accelerate-0.5.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.13.3 fsspec-2021.10.1 huggingface-hub-0.0.19 multidict-5.2.0 sentencepiece-0.1.96 xxhash-2.0.2 yarl-1.7.0\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-rapl2leh\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-rapl2leh\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (21.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (0.0.19)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (3.3.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 38.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 40.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers==4.12.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.12.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.12.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (2021.5.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.0.dev0) (1.15.0)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.12.0.dev0-py3-none-any.whl size=2993390 sha256=630a752c4bc192a9736f375e227e57ef835b543be595a010c04ba0fa55371a37\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-me3d9lhh/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n",
            "Successfully built transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vRpwDnCiFC2",
        "outputId": "4a8616eb-0b8d-41ef-ee5d-66d89df743f7"
      },
      "source": [
        "!huggingface-cli login"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "        \n",
            "Username: gagan3012\n",
            "Password: \n",
            "Login successful\n",
            "Your token has been saved to /root/.huggingface/token\n",
            "\u001b[1m\u001b[31mAuthenticated through git-crendential store but this isn't the helper defined on your machine.\n",
            "You will have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal to set it as the default\n",
            "\n",
            "git config --global credential.helper store\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjdNu36CiR4x",
        "outputId": "550ee6c0-a81c-436f-decc-fbbdddc2ee74"
      },
      "source": [
        "!sudo apt-get install git-lfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 2,129 kB of archives.\n",
            "After this operation, 7,662 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2,129 kB]\n",
            "Fetched 2,129 kB in 1s (1,844 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package git-lfs.\n",
            "(Reading database ... 155047 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\n",
            "Unpacking git-lfs (2.3.4-1) ...\n",
            "Setting up git-lfs (2.3.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac4ie0RFex4L",
        "outputId": "7f80d7aa-425a-49fd-f1d3-fd2da71de66b"
      },
      "source": [
        "!python run_clm.py \\\n",
        "    --model_name_or_path distilgpt2 \\\n",
        "    --train_file /content/train.txt \\\n",
        "    --validation_file /content/test.txt \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir /content/model \\\n",
        "    --push_to_hub"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/18/2021 17:54:28 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "10/18/2021 17:54:28 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/model/runs/Oct18_17-54-28_a3a7203bab16,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=/content/model,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=True,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/model,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "10/18/2021 17:54:28 - WARNING - datasets.builder - Using custom data configuration default-5896ebedd4144666\n",
            "10/18/2021 17:54:28 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "10/18/2021 17:54:28 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-5896ebedd4144666/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
            "10/18/2021 17:54:28 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-5896ebedd4144666/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
            "10/18/2021 17:54:28 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-5896ebedd4144666/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
            "\r  0% 0/2 [00:00<?, ?it/s]\r100% 2/2 [00:00<00:00, 856.42it/s]\n",
            "[INFO|configuration_utils.py:588] 2021-10-18 17:54:29,120 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|configuration_utils.py:625] 2021-10-18 17:54:29,121 >> Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.12.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:341] 2021-10-18 17:54:29,255 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:588] 2021-10-18 17:54:29,390 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|configuration_utils.py:625] 2021-10-18 17:54:29,391 >> Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.12.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-10-18 17:54:30,316 >> loading file https://huggingface.co/distilgpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-10-18 17:54:30,317 >> loading file https://huggingface.co/distilgpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-10-18 17:54:30,317 >> loading file https://huggingface.co/distilgpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-10-18 17:54:30,317 >> loading file https://huggingface.co/distilgpt2/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-10-18 17:54:30,317 >> loading file https://huggingface.co/distilgpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-10-18 17:54:30,317 >> loading file https://huggingface.co/distilgpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:588] 2021-10-18 17:54:30,453 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|configuration_utils.py:625] 2021-10-18 17:54:30,454 >> Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.12.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1340] 2021-10-18 17:54:30,677 >> loading weights file https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
            "[INFO|modeling_utils.py:1605] 2021-10-18 17:54:32,112 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1614] 2021-10-18 17:54:32,112 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "10/18/2021 17:54:32 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-5896ebedd4144666/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-762caffb5960ef53.arrow\n",
            "10/18/2021 17:54:32 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-5896ebedd4144666/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-52efc77412bb939f.arrow\n",
            "10/18/2021 17:54:32 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-5896ebedd4144666/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-2bbd0cb143fd3860.arrow\n",
            "10/18/2021 17:54:32 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-5896ebedd4144666/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-7644aabc3e65e168.arrow\n",
            "Cloning https://huggingface.co/gagan3012/model into local empty directory.\n",
            "10/18/2021 17:54:36 - WARNING - huggingface_hub.repository - Cloning https://huggingface.co/gagan3012/model into local empty directory.\n",
            "[INFO|trainer.py:1196] 2021-10-18 17:54:40,672 >> ***** Running training *****\n",
            "[INFO|trainer.py:1197] 2021-10-18 17:54:40,672 >>   Num examples = 13\n",
            "[INFO|trainer.py:1198] 2021-10-18 17:54:40,672 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1199] 2021-10-18 17:54:40,672 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1200] 2021-10-18 17:54:40,672 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1201] 2021-10-18 17:54:40,672 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1202] 2021-10-18 17:54:40,672 >>   Total optimization steps = 6\n",
            "  0% 0/6 [00:00<?, ?it/s]Traceback (most recent call last):\n",
            "  File \"run_clm.py\", line 524, in <module>\n",
            "    main()\n",
            "  File \"run_clm.py\", line 472, in main\n",
            "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1316, in train\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1849, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1881, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1055, in forward\n",
            "    return_dict=return_dict,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 894, in forward\n",
            "    output_attentions=output_attentions,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 401, in forward\n",
            "    output_attentions=output_attentions,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 336, in forward\n",
            "    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 193, in _attn\n",
            "    attn_weights = torch.matmul(query, key.transpose(-1, -2))\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 11.17 GiB total capacity; 10.30 GiB already allocated; 282.81 MiB free; 10.35 GiB reserved in total by PyTorch)\n",
            "  0% 0/6 [00:00<?, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJtAORK5e-Fh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}